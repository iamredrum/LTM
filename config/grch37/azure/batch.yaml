
storage_container_name: 'tasks-container-titan'


pools:
  titangrch37standard:
    pool_vm_size: 'STANDARD_A2M_V2'
    node_os_publisher: 'Canonical'
    node_os_offer: 'UbuntuServer'
    node_os_sku: '16'
    data_disk_sizes:
      1: 1000
    max_tasks_per_node: 2
    auto_scale_formula: |
      numAddMax = 20;
      numDelMax = 20;
      startingNumberOfVMs = 1;
      minNumberofVMs = 1;
      maxNumberofVMs = 1000;
      pendingTaskSamplePercent = $PendingTasks.GetSamplePercent(180 * TimeInterval_Second);
      pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg($PendingTasks.GetSample(180 * TimeInterval_Second));
      cores = $TargetLowPriorityNodes * 2;
      $extraVMs =  (pendingTaskSamples - cores) / 2;
      $extraVMs = min(numAddMax, $extraVMs);
      $extraVMs = max(-numDelMax, $extraVMs);
      targetVMs = ($TargetLowPriorityNodes + $extraVMs);
      $TargetLowPriorityNodes = max(minNumberofVMs,min(maxNumberofVMs, targetVMs));
    # Create VM
    # ---------
    # Install pip, singularity
    # Download the single cell pipeline data to the batch shared directory
    # including the reference data and the singularity container
    create_vm_commands: |
      sudo sh startup.sh 1 {accountname} {accountkey} reference-grch37-titan
    start_resources:
      startup.sh: /startup-resources-titan/startup.sh

  titangrch37multicore:
    pool_vm_size: 'STANDARD_D14_V2'
    node_os_publisher: 'Canonical'
    node_os_offer: 'UbuntuServer'
    node_os_sku: '16'
    data_disk_sizes:
      1: 1000
    max_tasks_per_node: 1
    auto_scale_formula: |
      numAddMax = 20;
      numDelMax = 20;
      startingNumberOfVMs = 0;
      minNumberofVMs = 0;
      maxNumberofVMs = 1000;
      pendingTaskSamplePercent = $PendingTasks.GetSamplePercent(180 * TimeInterval_Second);
      pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg($PendingTasks.GetSample(180 * TimeInterval_Second));
      cores = $TargetDedicatedNodes;
      $extraVMs =  (pendingTaskSamples - cores);
      $extraVMs = min(numAddMax, $extraVMs);
      $extraVMs = max(-numDelMax, $extraVMs);
      targetVMs = ($TargetDedicatedNodes + $extraVMs);
      $TargetDedicatedNodes = max(minNumberofVMs,min(maxNumberofVMs, targetVMs));
    # Create VM
    # ---------
    # Install pip, singularity
    # Download the single cell pipeline data to the batch shared directory
    # including the reference data and the singularity container
    create_vm_commands: |
      sudo sh startup.sh 1 {accountname} {accountkey} reference-grch37-titan
    start_resources:
      startup.sh: /startup-resources-titan/startup.sh

  titangrch37highmem:
    pool_vm_size: 'STANDARD_A7'
    node_os_publisher: 'Canonical'
    node_os_offer: 'UbuntuServer'
    node_os_sku: '16'
    data_disk_sizes:
      1: 1000
    max_tasks_per_node: 1
    auto_scale_formula: |
      numAddMax = 20;
      numDelMax = 20;
      startingNumberOfVMs = 0;
      minNumberofVMs = 0;
      maxNumberofVMs = 1000;
      pendingTaskSamplePercent = $PendingTasks.GetSamplePercent(180 * TimeInterval_Second);
      pendingTaskSamples = pendingTaskSamplePercent < 70 ? startingNumberOfVMs : avg($PendingTasks.GetSample(180 * TimeInterval_Second));
      cores = $TargetLowPriorityNodes;
      $extraVMs =  (pendingTaskSamples - cores);
      $extraVMs = min(numAddMax, $extraVMs);
      $extraVMs = max(-numDelMax, $extraVMs);
      targetVMs = ($TargetLowPriorityNodes + $extraVMs);
      $TargetLowPriorityNodes = max(minNumberofVMs,min(maxNumberofVMs, targetVMs));
    create_vm_commands: |
      sudo sh startup.sh 1 {accountname} {accountkey} reference-grch37-titan
    start_resources:
      startup.sh: /startup-resources-titan/startup.sh

# Compute Task Start Commmands
# ----------------------------
# Development specific command
# Make a development directory including site-packages
# Pull the latest code from the pypeliner and single cell repo
# and make sure they are used by the singularity container
compute_start_commands: |
  clean_up () {
    echo "clean_up task executed"
    find $AZ_BATCH_TASK_WORKING_DIR/ -xtype l -delete
    exit 0
  }
  trap clean_up EXIT
  mkdir -p /datadrive/$AZ_BATCH_TASK_WORKING_DIR/data-titan/
  mkdir -p /datadrive/$AZ_BATCH_TASK_WORKING_DIR/results-titan/
  mkdir -p /datadrive/$AZ_BATCH_TASK_WORKING_DIR/temp-titan/
  mkdir -p /datadrive/$AZ_BATCH_TASK_WORKING_DIR/logs-titan/
  mkdir -p /datadrive/$AZ_BATCH_TASK_WORKING_DIR/pipeline-titan/

# Compute Task Run Commmand
# ----------------------------
# Run the singularity container, mapping the development directory
# to the development directory of the container, the singlecellpipeline
# directory to the ref data directory, and the working directory to
# the /datadrive directory for common paths between head vm and compute
# vm.  Run the delegator command in the container.
compute_run_command: >
  singularity
  exec
  -B /datadrive/$AZ_BATCH_TASK_WORKING_DIR:/data-titan/
  -B /datadrive/$AZ_BATCH_TASK_WORKING_DIR:/results-titan/
  -B /datadrive/$AZ_BATCH_TASK_WORKING_DIR:/temp-titan/
  -B /datadrive/$AZ_BATCH_TASK_WORKING_DIR:/logs-titan/
  -B /datadrive/$AZ_BATCH_TASK_WORKING_DIR:/pipeline-titan/
  -B /refdata/:/refdata/
  -H $AZ_BATCH_TASK_WORKING_DIR/
  --pwd $AZ_BATCH_TASK_WORKING_DIR/
  /refdata/test_titan.img
  pypeliner_delegate
  {input_filename}
  {output_filename}

# Compute Task Finish Commmand
# ----------------------------
# Post command workaround because azure batch fails to upload outputfiles
# if there are broken symlinks in the working directory.
compute_finish_commands: >
  find /datadrive/$AZ_BATCH_TASK_WORKING_DIR/ -xtype l -delete

no_delete_pool: True
no_delete_job: False
